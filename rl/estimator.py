import torch
import torch.nn as nn

class Estimator():
    def __init__(self, state_shape_size: int, num_actions: int, hidden_layers: [int], learning_rate: int):

        self.qnet = EstimatorNetwork(state_shape_size, num_actions, hidden_layers)

        self.optimizer = torch.optim.Adam(self.qnet.parameters(), learning_rate)
        self.loss = torch.nn.MSELoss(reduction='sum')

    def predict_nograd(self, state):
        with torch.no_grad():
            return self.qnet.forward(state)

    def update(self, state, actions, target_actions):
        """
        Updates the q-network after prediction
        :param state: current state
        :param actions:
        :param target_actions: actions generated by the target network
        :return:
        """
        self.optimizer.zero_grad()  # zeroes the gradient

        self.qnet.train()  # sets up network for training (not really necessary but helpful)

        actions = self.qnet.forward(state)

        # TODO: see if this type of training is actually real (may have to change actions??)
        batch_loss = self.loss(actions, target_actions)
        batch_loss.backward()
        self.optimizer.step()

        self.qnet.eval()  # sets up network for evaluation (not really necessary but probably safer)

"""
Based on RLCard DQN implementation
https://github.com/datamllab/rlcard/blob/master/rlcard/agents/dqn_agent.py#L413

Fully connected neural net with ReLU activation functions
Input: State
Output: Actions
"""
class EstimatorNetwork(nn.Module):
    def __init__(self, state_shape_size: int, num_actions: int, hidden_layers: [int]):
        """
        :param state_shape_size: state shape size
        :param num_actions: number of actions
        :param hidden_layers: size of each of the hidden layers
        """
        self.state_shape_size = state_shape_size
        self.num_actions = num_actions
        self.hidden_layers = hidden_layers

        nodes_per_layer = [self.state_shape] + hidden_layers

        network = []
        for i in range(len(nodes_per_layer) - 1):
            network.append(nn.Linear(nodes_per_layer[i], nodes_per_layer[i + 1]))
            network.append(nn.ReLU())

        network.append(nn.Linear(nodes_per_layer[-1], num_actions))

        self.model = nn.Sequential(*network)

    def forward(self, x):
        """
        Passes an input tensor through the net

        :param x: the input tensor
        :return: output of the model
        """
        return self.model(x)